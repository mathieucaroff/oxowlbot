"""
This type stub file was generated by pyright.
"""

import logging
import torch.nn as nn
from typing import Any, Optional

"""
Pytorch implementation of basic sequence to Sequence modules.
"""
logger = logging.getLogger('stanza')
class BasicAttention(nn.Module):
    """
    A basic MLP attention layer.
    """
    def __init__(self, dim):
        self.linear_in = ...
        self.linear_c = ...
        self.linear_v = ...
        self.linear_out = ...
        self.tanh = ...
        self.sm = ...
    
    def forward(self, input, context, mask: Optional[Any] = ..., attn_only: bool = ...):
        """
        input: batch x dim
        context: batch x sourceL x dim
        """
        ...
    


class SoftDotAttention(nn.Module):
    """Soft Dot Attention.

    Ref: http://www.aclweb.org/anthology/D15-1166
    Adapted from PyTorch OPEN NMT.
    """
    def __init__(self, dim):
        """Initialize layer."""
        self.linear_in = ...
        self.sm = ...
        self.linear_out = ...
        self.tanh = ...
        self.mask = ...
    
    def forward(self, input, context, mask: Optional[Any] = ..., attn_only: bool = ...):
        """Propogate input through the network.

        input: batch x dim
        context: batch x sourceL x dim
        """
        ...
    


class LinearAttention(nn.Module):
    """ A linear attention form, inspired by BiDAF:
        a = W (u; v; u o v)
    """
    def __init__(self, dim):
        self.linear = ...
        self.linear_out = ...
        self.sm = ...
        self.tanh = ...
        self.mask = ...
    
    def forward(self, input, context, mask: Optional[Any] = ..., attn_only: bool = ...):
        """
        input: batch x dim
        context: batch x sourceL x dim
        """
        ...
    


class DeepAttention(nn.Module):
    """ A deep attention form, invented by Robert:
        u = ReLU(Wx)
        v = ReLU(Wy)
        a = V.(u o v)
    """
    def __init__(self, dim):
        self.linear_in = ...
        self.linear_v = ...
        self.linear_out = ...
        self.relu = ...
        self.sm = ...
        self.tanh = ...
        self.mask = ...
    
    def forward(self, input, context, mask: Optional[Any] = ..., attn_only: bool = ...):
        """
        input: batch x dim
        context: batch x sourceL x dim
        """
        ...
    


class LSTMAttention(nn.Module):
    r"""A long short-term memory (LSTM) cell with attention."""
    def __init__(self, input_size, hidden_size, batch_first: bool = ..., attn_type=...):
        """Initialize params."""
        self.input_size = ...
        self.hidden_size = ...
        self.batch_first = ...
        self.lstm_cell = ...
    
    def forward(self, input, hidden, ctx, ctx_mask: Optional[Any] = ...):
        """Propogate input through the network."""
        ...
    


